# Loppuraportti
## Ohjelman sekvenssikaavio ja yleiskuvaus

TODO: litetään tähän mermaid-sekvenssikaaviokuva

Ohjelma muuntaa raakadatan (.pcapang) ensin tekstimuotoiseksi taulukoksi (.tsv). Tekstimuotoinen data ajetaan dekooderin läpi. Tästä syntyy yksittäiset signaalit ja niiden arvot sisältävä tiedosto (.json) ja lisäksi tilastoja sisältävä tiedosto (.csv)

### raakadatan purku (tshark-konversio)

Huom.: Tässä esitetty tieto raakadatan formaatista koskee vain projektissa käytettyä esimerkkitiedostoa  can_dump.pcapng (md5sum 5816a743c69c602b28c3645ccf1493a0).

Raakadatasta löytyy kaksi olennaista tietoa, .canId ja .data. Näistä canId kertoo, mikä ECU on kyseessä, ja data sisältää varsinaisen datan.

Varsinaisen datan lisäksi .pcapng -paketit sisältävät paketinkaappaukseen liittyvää metadataa. Näistä on hydynnetty vain aikaleimaa.  Se löytyy sekä ihmisen luettavassa että koneluettavassa muodossa. Datan jatkokäsittelyn helpottamiseksi on käyttöön valittu koneluettava unix epoch.


Raakadataa voi lukea esimerkiksi wiresharkin cli-työkalu tsharkilla. Pythonilla voisi käyttää vaikkapa pyshark-kirjastoa. Kokeilimme molempia vaihtoehtoja, ja totesimme tsharkin olevan noin 10 kertaa nopeampi. Suuremmilla datamäärillä nopeusero olisi tshark-toteutuksen eduksi merkittävä. Tshark-toteutuksen haittapuolia tunnistettiin kaksi:
1. windowsille ja linuxille jouduttiin tekemään hieman erilaiset komennot ja tsharkin asennusohjeet
2. Joudutaan tekemään ylimääräinen työvaihe ja väliaikainen .tsv-tiedosto, joka vie levytilaa. Toisaalta väliaikaisen tiedoston etuna on, että raakadatan purku voidaan tehdä kerran, ja sitä voidaan hyödyntää tulevilla ajokerroilla. Näin useamman analyysin teko samasta aineistosta nopeutuu.

Nopeustestin tulokset 100 000 rivin testitiedostolla:
Pyshark-toteutus dekoodaukseen:
% time python3 src/main.py -i src/dump100000.pcapng <...> 17.77s user 0.90s system 103% cpu 18.049 total

tshark-toteutus dekoodaukseen ja erillisen .tsv-tiedoston lukeminen:
% time ./src/raakadata_konversio.sh src/dump100000.pcapng konvertoitu_100000.tsv  0.56s user 0.07s system 97% cpu 0.648 total
% time python3 src/main.py -i konvertoitu_100000.tsv -d <...>  0.91s user 0.12s system 98% cpu 1.050 totali


Raakadatan purkamisen lopputuloksena luodaan .tsv tässä formaatissa (headereita ei synny lopulliseen tiedostoon, ne ovat tässä vain dokumentoinnin vuoksi):
timestamp (unix epoc)   CanID                                                           Data
1736342840.295986390    Controller Area Network, Ext. ID: 166 (0x000000a6), Length: 8   00000040000000ee

Jatkokehitysidea: .tsv-tiedostossa CanID -kentästä tarvitaan vain ID (yllä 166). Tiedostokokoa saadaan pienemmäksi filtteröimällä turhat merkit pois jo tässä vaiheessa, ja muuttamalla dekooderissa olevaa parserin toteutusta.

## dekoodaus ja dbc-tiedosto
		
Purettu raakadata konvertoidaan ihmisen ymmärrettävissä olevaksi informaatioksi dekoodaamalla se ajoneuvon mallikohtaista .dbc -tiedostoa vasten.

Pythonin cantools-kirjasto dekoodaa raakadatan .canId -kentästä ECU:n nimen (cantools-käsittein "messaga name", ja .data -kentästä signaalin osat.

Lisätietoa:
https://github.com/commaai/opendbc
https://pypi.org/project/cantools/
https://www.csselectronics.com/pages/can-dbc-file-database-intro

## Projektin aikana opittuja asioita

### tekniset valinnat ja mahdolliset rajoitteet
Ensimmäinen versio toteutuksesta noudatti pipeline-arkkitehtuuria; meillä oli sekalainen kasa skriptejä, joilla tehtiin ohjelman suorituksen vaiheet erillisinä, osina. Välissä tieto tallennettiin tiedostoiksi.  Ratkaisu toimi alussa, mutta lisäominaisuuksien kehittäminen tiimityönä osoittautui hankalaksi koordinoida. Yhtenäinen python-projekti osoittautui helpommaksi kehitystyön kannakta. Pythonin valikoituminen toteutuskieleksi johtui puhtaasti siitä, että dbc-konversioon löytyi nopeasti helppokäyttöinen cantools-kirjasto.

Projektia toteuttaessa ei pohdittu toteutuksen resurssinkäyttöä. Käytössä ollut esimerkkitiedosto kääntyi kehittäjien koneilla muutamissa minuuteissa. 

Isoilla datamäärillä täytyy huomioida käytettävissä olevan muistin rajallisuus. Toteutuksemme on kovakoodatusti rajattu käyttämään maksimissaan n. 3 gigaa muistia. Toteutuksessa on omat ongelmansa ja paljon huomioimattomia rajatapuksia. Ratkaisun on tarkoitus mahdollistaa rajattoman suuren datamäärän käsittely. Teknisesti toteutus toimii lukemalla käsittelyä varten noin 3g dataa kerralla sisään. Ratkaisu toimii dekoodatun json-tiedoston tuottamiseen. Ratkaisu ei kuitenkaan ole hyvä tilastointiin. Tiettyjä tunnuslukuja, kuten mediaani, ei voida laskea osissa.

Vaihtoehtoinen toteutustapa tilastojen laskemiseen olisi palata pipeline-toteutukseen, jossa ensin luotaisiin dekoodattu json-tiedosto, josta sitten käsiteltäisiin signaaalit yksi signaalityyppi kerrallaan. Näin muistissa kerrallaan olevan datan määrää saataisiin pienennettyä, ja mahdollisesti laskettua esimerkiksi mediaaniarvoja. Teorian tasolla ongelma on edelleen sama; mediaanin laskemiseksi muistissa on oltava kaikki ksittäiset arvot kerralla. Toinen ratkaisutapa olisi tutustua matemaattisiin ratkaisuihin tarkemmin, ja esittää tarkan mediaanin sijaan valistunut arvaus. Tämä tutkintalinja päätettiin ajanpuutteen vuoksi jättää tarkemmin selvittämättä.

Yhteenvetona teknisistä valinnoista voidaan todeta, että määrittely ja käyttötapausten tarkempi tunteminen on jäänyt projektissa puutteelliseksi.

### VSS
Yhtenä tehtävänä oli tutustua VSS-syntaksiin. Toteutimme mvp-tyylisen ominaisuuden. Dekooderin valitsimella --vss tehdään signaaleille konversio DBC-muodosta VSS-muotoon.

VSS:n mukaisia tietoja ei voi suoraan päivittää DBC-tiedostoon, koska DBC-formaatti ei tue pisteitä signaalien mimissä. VSS-muodossa taas pisteet ovat olennainen osa syntaksia.

Muunnos on toteutettu käsin tehdyllä mäppäyksellä, joka ylikirjoittaa DBC-signaalin nimen VSS:n mukaisella nimellä. Mäppäystalukon arvot ovat valistuneita arvauksia. Taulukko on puutteellinen.

Mäppäyksen parantaminen vaatisi DBC-tiedoston parempaa tuntemista ja mahdollisia yksittäisten signaalinen konversioita. Esimerkkiksi Chassis.Chassis.Brake.PedalPosition sisältää arvon prosenteissa välillä 0-100.  DBC-tiedostossa oleva BRAKE.BRAKE_PEDAL sisältää arvon välillä 0-255. BRAKE_PEDAL -arvo saattaisi olla suoraan muunnettavissa prosenteiksi, mutta arvo voi ECU:sta riippuen tarkoittaa myös jotain muuta.

Jatkokehitystarve: DBC-formaatissa on message name ("BO_") ja signaali ("SG_"). Mäppäys tehdään nyt ainoastaan signaalin perusteella. Esimerkkinä käytetyssä DBC-tiedostossa on muutamia signaaleita, jotka ovat käytössä useammassa viestissä.


Lisätietoa:
https://github.com/COVESA/vehicle_signal_specification



### Diffpriv
Yhtenä tehtävänä oli tutkia diffpriv -menetelmiä. Toteutimme valitsimen --diffpriv, jolla lasketaan mediaaneja diffpriv-menetelmällä. 

Tarkempi raportti on tehty erillisessä repossa: https://github.com/jaacoppi/canbusdp

## Sekalaisia jatkokehitysideoita
- Ohjelma toimii toistaiseksi muistinvaraisesti. Muisti loppuu kesken, mikäli input-tiedosto on merkittävästi suurempi kuin käytettävissä oleva muisti. Keskeisiä refaktorointikohteita on .json -tiedoston tallennuksen muuttaminen niin, että decoded_lines -muuttujaa ei tallenneta. Toisaalta vaatisi muutoksia myös tilastointikoodiin
- DBC-tiedosto on kaiken dekoodauksen avain; sen tarkempi ymmärtäminen on välttämätöntä tulosten verifioimiseksi
- Työkalun käyttö oikealla datalla ja oikeilla käyttötapauksilla, ja sitä kautta kehityskohteiden listaus (ainakin suorituksen optimointi ja tutkijoiden workflow:n ymmärtäminen)
- erillinen tshark-konversioskripti voitaisiin toteuttaa käyttäen pythonin subprocess -komentoa. Näin työkalusta saataisiin käyttöliittymältään yhtenäisempi. Toteutuksessa tulee huomioida käyttöjärjestelmäkohtaiset erot.
- työkalun tuottamaan output-jsoniin olisi syytä lisätä metadataa siitä, millä työkalun versiolla, valitsimilla ja lähdedatalla tulos on syntynyt. Tämä helpottaa tulosten versionhallintaa ja tutkimuksen toistettavutta.

- raakadatan filtteröinti ennen dekoodausta:
Yhtenä tutkimuskysymyksenä oli, voidaanko raakadataa filtteröidä. Tutkimuksemme perusteella datan tuottamisvaiheessa se on hankalaa. Tshark käyttää syötteenä kaapattuja paketteja. Jos tämä olisi internet-liikennettä, metatiedoista löytyisi esimerkiksi käytetty protokolla, kohde- ja lähdeosoitteet. Näiden perusteella voitaisiin toteuttaa filtteröintiä.

Meidän käyttötapauksessamme ainoa filtteröinnin perusteella käytettävä tieto on CanID-numero. Sitä ei onnistuttu filtteröimään suoraan tsharkilla. Filtteröinti olisi yksinkertaista tehdä esim. linux grepillä. Kuitenkin huomattiin, että CanID ei ole ihmisen ymmärtämässä muodossa ennen dekoodausvaihetta, jolloin se mäpätään .dbc -tiedostossa olevaan signaaliin. Näin ollen filtteröinti voidaan tehdä dekoodausvaiheessa.

Jatkokehitysideana filtterityökalu, joka hakee ID:n .dbc -tiedostosta nimen perusteella ja filtteröi konvertoitua raakadataa. Näin saadaan vähennettyä raakadatatiedoston kokoa. Nopeasti toteutettu kokeilu näyttää toimivan, mutta poikkeustapausten käsittelyä ei ole mietitty:
 % export KEY=BRAKE                                       
% export KEY2BO=$(grep BO_ data/toyota_rav4_hybrid_2017_pt_generated.dbc| grep " ${KEY}:" | cut -d ' ' -f 2)
% grep "ID: ${KEY2BO}" konvertoitu_raakadata.tsv > filtteröity_raakadata.tsv


### Automaattitestit
Projektin tuotokselle luotiin automaattitestit, jotka testaa muutamaa toimivuutta, mutta eivät kata riittävällä tasolla tuotoksen toimivuutta.
    - Tämä johtui siitä, että automaattitestien luominen jäi projektin loppupuolelle ja jouduttiin tekemään kiireessä, ja täten pakottaen ne jäämään mvp-tasolle, jotta näyttäisimme ymärtänemme automaattitestien tarpeen vaikka huomasimmekin näiden tarpeen liian myöhässä projektin elinkaarta miettien.
Tästä syystä testejä tulisi kehittää pitemmälle ja testitapauksien määrää tulisi kasvattaa tekemään automaattitesteistä kattavampia.

Tällä hetkellä automaattitestit testaavat muutamia toiminnallisuuksia, kuten joitain tulosteita ja parse_canID oikeellisuutta.   

Opittua
- Automaattitestausta tulisi aloittaa aiemmassa vaiheessa projektia 
- Testejä tulisi ylläpitää koko kehityksen ajan, ja luoda uusia testejä uusien toiminnalisuuksien kehittyessä.
- Automaattitestien kattavuuden tulisi kattaa koko tai merkittävimmät toiminnallisuudet ohjelmasta.

Lisätietoa:
https://wiki.wireshark.org/Development/PcapNg
https://pypi.org/project/pyshark/
https://github.com/KimiNewt/pyshark/issues/357


